{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the modules we're going to need\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd# Added\n",
    "from IPython.display import Audio\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've previously preprocessed our data and coverted all files to a sample rate of 44100\n",
    "samplerate = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify where the audio files for training and testing reside\n",
    "train_folder = './mir_class_train'\n",
    "test_folder = './mir_class_test'\n",
    "\n",
    "# Get a list of all the training audio files (must be .WAV files)\n",
    "train_files = glob.glob(os.path.join(train_folder, '*.wav'))\n",
    "\n",
    "# Get a list of all the test audio files (must be .WAV files)\n",
    "test_files = glob.glob(os.path.join(test_folder, '*.wav'))\n",
    "\n",
    "# Specify the labels (classes) we're going to classify the data into\n",
    "label0 = 'abscent'\n",
    "label1 = 'present'\n",
    "labels = [label0, label1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make 1 second summarization as features with half second of hop length\n",
    "# 172 frames == 1 second (using 44100 samples per second)\n",
    "feature_length = 172\n",
    "half_sec = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: StrandOfOaks_Spacestation_MIX.wav\n",
      "file label size: 507\n",
      " \n",
      "filename: PurlingHiss_Lolita_MIX.wav\n",
      "file label size: 532\n",
      " \n",
      "filename: TheSoSoGlos_Emergency_MIX.wav\n",
      "file label size: 346\n",
      " \n",
      "filename: TheScarletBrand_LesFleursDuMal_MIX.wav\n",
      "file label size: 631\n",
      " \n",
      "filename: PortStWillow_StayEven_MIX.wav\n",
      "file label size: 248\n",
      " \n",
      "filename: MusicDelta_Rock_MIX.wav\n",
      "file label size: 26\n",
      " \n",
      "filename: Snowmine_Curfews_MIX.wav\n",
      "file label size: 572\n",
      " \n",
      "filename: StevenClark_Bounty_MIX.wav\n",
      "file label size: 602\n",
      " \n",
      "filename: Wolf_DieBekherte_MIX.wav\n",
      "file label size: 392\n",
      " \n",
      "filename: SecretMountains_HighHorse_MIX.wav\n",
      "file label size: 739\n",
      " \n",
      "filename: MusicDelta_Reggae_MIX.wav\n",
      "file label size: 35\n",
      " \n",
      "filename: NightPanther_Fire_MIX.wav\n",
      "file label size: 442\n",
      " \n",
      "filename: Schumann_Mignon_MIX.wav\n",
      "file label size: 546\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the training features and corresponding training labels\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in train_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load VGGish audio embeddings\n",
    "    vggish = pd.read_csv(tf[:-7]+\"VGGish.csv\",index_col=None, header=None)\n",
    "    vggish = pd.DataFrame.as_matrix(vggish)\n",
    "    \n",
    "    # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"MELODY1.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)[:,1]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    #print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "    for idx in range(vggish.shape[0]):\n",
    "        start = idx*half_sec\n",
    "        #mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        #mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(vggish[idx])\n",
    "        #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) > half_sec:\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "        \n",
    "    #tf_label = ['present' if x > 0.0 else 'abscent' for x in f0line]\n",
    "\n",
    "    # Get labels index\n",
    "    tf_label_ind = [labels.index(lbl) for lbl in tf_label]\n",
    "    print(\"file label size: {:d}\".format(len(tf_label_ind)))\n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)-1):\n",
    "        train_features.append(feature_vector[idx])\n",
    "        train_labels.append(tf_label_ind[idx]) # Labels are on double rate\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: SweetLights_YouLetMeDown_MIX.wav\n",
      "file label size: (815,)\n",
      " \n",
      "filename: TheDistricts_Vermont_MIX.wav\n",
      "file label size: (474,)\n",
      " \n",
      "filename: MusicDelta_Rockabilly_MIX.wav\n",
      "file label size: (53,)\n",
      " \n",
      "filename: Schubert_Erstarrung_MIX.wav\n",
      "file label size: (361,)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the test features and corresponding test labels\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in test_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load VGGish audio embeddings\n",
    "    vggish = pd.read_csv(tf[:-7]+\"VGGish.csv\",index_col=None, header=None)\n",
    "    vggish = pd.DataFrame.as_matrix(vggish)\n",
    "    \n",
    "    # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"MELODY1.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)[:,1]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    #print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "    for idx in range(vggish.shape[0]):\n",
    "        start = idx*half_sec\n",
    "        #mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        #mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(vggish[idx])\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) > half_sec:\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "    \n",
    "    #Get labels index\n",
    "    tf_label_ind = np.array([labels.index(lbl) for lbl in tf_label])\n",
    "    print(\"file label size: {}\".format(tf_label_ind.shape))\n",
    "    \n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)-1):\n",
    "        test_features.append(feature_vector[idx])\n",
    "        test_labels.append(tf_label_ind[idx])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scale object\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Learn the parameters from the training data only\n",
    "scaler.fit(train_features)\n",
    "\n",
    "# Apply the learned parameters to the training and test sets:\n",
    "train_features_scaled = scaler.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Note, the first 2 operations (learning the standardization parameters from the training data \n",
    "# and applying them to the the training data) can be performed in one line using:\n",
    "# train_features_scaled = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_VGGish.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the scaler to disk\n",
    "filename = 'scaler_VGGish.sav'\n",
    "joblib.dump(scaler, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Step 3: model training</h2>\n",
    "\n",
    "Now that all of our features are computed, we can train a clasification model! In this example we're going to use the following model: the support vector machine classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use scikit-learn to train a model with the training features we've extracted\n",
    "\n",
    "# Lets use a SVC with default parameters: kernel RBF \n",
    "clf = sklearn.svm.SVC()\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf.fit(train_features_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model_VGGish.sav']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_VGGish.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf.fit(train_features_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5750441436138906\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[520 415]\n",
      " [289 475]]\n"
     ]
    }
   ],
   "source": [
    "# lets compute the show the confusion matrix:\n",
    "cm = sklearn.metrics.confusion_matrix(test_labels, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
