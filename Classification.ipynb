{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Voice Activity Detector based on MFCC features</h1>\n",
    "\n",
    "This notebook shows how to build a Voice Activity classifier using MFCC features. \n",
    "\n",
    "**Why MFCC?**\n",
    "\n",
    "MFCC are a good starting point when building an audio classifier designed to distinguish between audio recordings based on their **timbre**, since they provide a compact representation of the spectral envelope. Examples of audio classses that are well characterized by their timbre include musical instruments (e.g. trumpet vs. piano) or urban environmental sounds (e.g. siren vs. car honk vs. and jackhammer).\n",
    "\n",
    "However, it is important to bear in mind that not all audio-related classification tasks are timbre-related. For example, you might be interested in identifying (classifiying) the chords played in a music recording. What makes one chord different from another is **not its timbre**: the same chord can be played by different instruments with different timbres and it will still be the same chord. What makes chords different (among other factors) are the notes (pitches) they contain. Thus, a more suitable class of features for chord classification would be [chroma features](https://en.wikipedia.org/wiki/Chroma_feature).\n",
    "\n",
    "Since in this example we want to distinguish between human voice presence and abscence on a song (genres), which (usually) have different timbre, MFCC are probably a good choice for the task.\n",
    "\n",
    "**IMPORTANT**: This example makes one assumption:\n",
    "\n",
    "1. All the audio files have been preprocessed to ensure they are in the same format: single channel (mono) with a sample rate of 44100 Hz and a bit depth of 16 in wav format. If you haven't preprocessed your audio, consult the **```convert_audio_format```** notebook in this repository. Working with audio files in different formats can lead to unexpected (=undesired!) results.\n",
    "\n",
    "\n",
    "Dependencies:\n",
    "* numpy: http://www.numpy.org/\n",
    "* scikit-learn: http://scikit-learn.org/\n",
    "* librosa: http://librosa.github.io/librosa/\n",
    "* matplotlib: http://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the modules we're going to need\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd# Added\n",
    "from IPython.display import Audio\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 0: Compiling your dataset</h2>\n",
    "\n",
    "For creating this example I've used a collection of songs with human singers. All the files have been preprocessed using the ```convert_audio_format``` and ```rename_files_predix``` notebooks, and then divided into a train and test folder. There are 13 files in the train folder and 4 files in the test folder.\n",
    "\n",
    "Since we will use different files in class (your music!), the results displayed in this notebook will be different when we run it in class. {?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 1: Data cleaning</h2>\n",
    "\n",
    "As noted above, this notebook assumes data cleaning has already been performed: all files are in the same format (single channel [mono], 16-bit depth, 44100 sample rate, wav), and the class label of each file is indicated as a prefix followed by an underscore in the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We've previously preprocessed our data and coverted all files to a sample rate of 44100\n",
    "samplerate = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 2: Feature extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Specify where the audio files for training and testing reside\n",
    "train_folder = './mir_class_train'\n",
    "test_folder = './mir_class_test'\n",
    "\n",
    "# Get a list of all the training audio files (must be .WAV files)\n",
    "train_files = glob.glob(os.path.join(train_folder, '*.wav'))\n",
    "\n",
    "# Get a list of all the test audio files (must be .WAV files)\n",
    "test_files = glob.glob(os.path.join(test_folder, '*.wav'))\n",
    "\n",
    "# Specify the labels (classes) we're going to classify the data into\n",
    "label0 = 'abscent'\n",
    "label1 = 'present'\n",
    "labels = [label0, label1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to compute MFCC features using 40 mel bands, 40 DCT coefficients, keeping the bottom 13 MFCC coefficients after skipping the first coefficient (i.e. keep MFCC coefficieints 1-13, discard coefficients 0 and 14-39).\n",
    "\n",
    "Before training (and testing), we are going to standardize our features across each dimension (this is particularly important if you plan to use an SVM classifier for some kernel functions).\n",
    "\n",
    "**Tip 1**: we might be able to obtain better clasification by adding more features to our feature vector. For example, we could compute the delta (first derivative) of each MFCC coefficient time-series (```np.diff(mfcc)```) and compute summary statistics from this time series as well. We could even do the same for the delta-delta (second derivative). Another option would be to add more summary statistics such as the minimum, maximum, median, or higher-order moments such as skewness and kurtosis. Be careful though: if the dimensionality of your featue vector is too high you run the risk of overfitting your data, especially if your dataset is small!\n",
    "\n",
    "**Tip 2**: since the train/test sets used in this example are relatively small, and since we're only going to run through this example once, the features are computed on the fly in memory. If, however, you are working with a larger dataset (e.g. UrbanSound8K), feature extraction can take a considerable time. In this case, it's a good idea to store the features to disk, so that you can experiment with different classification models and classifier hyper-parameters without having to re-compute the features every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets start by defining the parameters of our MFCC features: \n",
    "* window (frame) size\n",
    "* hop size\n",
    "* number of mel bands (the number of DCT coefficients will be the same in this implementation) \n",
    "* number of MFCC coefficients to keep: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "hop_size = 256\n",
    "n_bands = 40\n",
    "n_mfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make 1 second summarization as features with half second of hop length\n",
    "# 172 frames == 1 second (using 44100 samples per second)\n",
    "feature_length = 172\n",
    "half_sec = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll extract features from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: StrandOfOaks_Spacestation_MIX.wav\n",
      "number of chunks 488\n",
      "file label size: 488\n",
      " \n",
      "filename: PurlingHiss_Lolita_MIX.wav\n",
      "number of chunks 513\n",
      "file label size: 513\n",
      " \n",
      "filename: TheSoSoGlos_Emergency_MIX.wav\n",
      "number of chunks 334\n",
      "file label size: 334\n",
      " \n",
      "filename: TheScarletBrand_LesFleursDuMal_MIX.wav\n",
      "number of chunks 608\n",
      "file label size: 608\n",
      " \n",
      "filename: PortStWillow_StayEven_MIX.wav\n",
      "number of chunks 635\n",
      "file label size: 635\n",
      " \n",
      "filename: MusicDelta_Rock_MIX.wav\n",
      "number of chunks 26\n",
      "file label size: 26\n",
      " \n",
      "filename: Snowmine_Curfews_MIX.wav\n",
      "number of chunks 551\n",
      "file label size: 551\n",
      " \n",
      "filename: StevenClark_Bounty_MIX.wav\n",
      "number of chunks 579\n",
      "file label size: 579\n",
      " \n",
      "filename: Wolf_DieBekherte_MIX.wav\n",
      "number of chunks 378\n",
      "file label size: 378\n",
      " \n",
      "filename: SecretMountains_HighHorse_MIX.wav\n",
      "number of chunks 712\n",
      "file label size: 712\n",
      " \n",
      "filename: MusicDelta_Reggae_MIX.wav\n",
      "number of chunks 34\n",
      "file label size: 34\n",
      " \n",
      "filename: NightPanther_Fire_MIX.wav\n",
      "number of chunks 426\n",
      "file label size: 426\n",
      " \n",
      "filename: Schumann_Mignon_MIX.wav\n",
      "number of chunks 526\n",
      "file label size: 526\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the training features and corresponding training labels\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in train_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(tf, sr=samplerate, mono=True)\n",
    "\n",
    "    # Extract mfcc coefficients (remember we will discard the first one)\n",
    "    # To see all the relevant kwarg arugments consult the documentation for\n",
    "    # librosa.feature.mfcc, librosa.feature.melspectrogram and librosa.filters.mel\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_fft=window_size, hop_length=hop_size,\n",
    "                                fmax=samplerate/2, n_mels=n_bands, n_mfcc=(n_mfcc + 1))\n",
    "          \n",
    "    # Discard the first coefficient\n",
    "    mfcc = mfcc[1:,:]\n",
    "    \n",
    "    # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"MELODY1.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)[:,1]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "    for chunk in range(int(mfcc.shape[1]/half_sec)):\n",
    "        start = chunk*half_sec\n",
    "        mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(np.concatenate((mfcc_means, mfcc_stddevs), axis=0))\n",
    "        #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) > half_sec:\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "        \n",
    "    #tf_label = ['present' if x > 0.0 else 'abscent' for x in f0line]\n",
    "\n",
    "    # Get labels index\n",
    "    tf_label_ind = [labels.index(lbl) for lbl in tf_label]\n",
    "    print(\"file label size: {:d}\".format(len(tf_label_ind)))\n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)-1):\n",
    "        train_features.append(feature_vector[idx])\n",
    "        train_labels.append(tf_label_ind[idx]) # Labels are on double rate\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, do do the same for the test data (of course we must extact exactly the same features for the training and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: SweetLights_YouLetMeDown_MIX.wav\n",
      "mfcc matrix shape: (13, 67526)\n",
      "number of chunks 785\n",
      "file label size: (785,)\n",
      " \n",
      "filename: TheDistricts_Vermont_MIX.wav\n",
      "mfcc matrix shape: (13, 39303)\n",
      "number of chunks 457\n",
      "file label size: (457,)\n",
      " \n",
      "filename: MusicDelta_Rockabilly_MIX.wav\n",
      "mfcc matrix shape: (13, 4471)\n",
      "number of chunks 51\n",
      "file label size: (51,)\n",
      " \n",
      "filename: Schubert_Erstarrung_MIX.wav\n",
      "mfcc matrix shape: (13, 29957)\n",
      "number of chunks 348\n",
      "file label size: (348,)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the test features and corresponding test labels\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in test_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(tf, sr=samplerate, mono=True)\n",
    "\n",
    "    # Extract mfcc coefficients (remember we will discard the first one)\n",
    "    # To see all the relevant kwarg arugments consult the documentation for\n",
    "    # librosa.feature.mfcc, librosa.feature.melspectrogram and librosa.filters.mel\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_fft=window_size, hop_length=hop_size,\n",
    "                                fmax=samplerate/2, n_mels=n_bands, n_mfcc=(n_mfcc + 1))\n",
    "          \n",
    "    # Discard the first coefficient\n",
    "    mfcc = mfcc[1:,:]\n",
    "    print(\"mfcc matrix shape: {}\".format(mfcc.shape))\n",
    "    \n",
    "     # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"MELODY1.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)[:,1]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "    for chunk in range(int(mfcc.shape[1]/half_sec)):\n",
    "        start = chunk*half_sec\n",
    "        mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(np.concatenate((mfcc_means, mfcc_stddevs), axis=0))\n",
    "        #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) > half_sec:\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "    \n",
    "    #Get labels index\n",
    "    tf_label_ind = np.array([labels.index(lbl) for lbl in tf_label])\n",
    "    print(\"file label size: {}\".format(tf_label_ind.shape))\n",
    "    \n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)-1):\n",
    "        test_features.append(feature_vector[idx])\n",
    "        test_labels.append(tf_label_ind[idx])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we'll appply some post-processing in the form of feature standardization. In our case we're going to standardize across dimensions: this means we compare the feature vector of all files **in the training set** and for each dimension we subtract the mean and divide by the standard deviation across all feature vectors.\n",
    "\n",
    "**Important**: it is incorrect to standardize the entire dataset (training and test) in one go, because that means we've \"looked\" at the test data. The correct way to perform standardization is to learn the parameters (mean/std. dev) from the training set only, and then apply exactly the same process to the test set (without examining the data). This can be accomplished easily using scikit-learn's StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scale object\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Learn the parameters from the training data only\n",
    "scaler.fit(train_features)\n",
    "\n",
    "# Apply the learned parameters to the training and test sets:\n",
    "train_features_scaled = scaler.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Note, the first 2 operations (learning the standardization parameters from the training data \n",
    "# and applying them to the the training data) can be performed in one line using:\n",
    "# train_features_scaled = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the scaler to disk\n",
    "filename = 'scaler.sav'\n",
    "joblib.dump(scaler, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> Step 3: model training</h2>\n",
    "\n",
    "Now that all of our features are computed, we can train a clasification model! In this example we're going to use the following model: the support vector machine classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use scikit-learn to train a model with the training features we've extracted\n",
    "\n",
    "# Lets use a SVC with default parameters: kernel RBF \n",
    "clf = sklearn.svm.SVC()\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf.fit(train_features_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf.fit(train_features_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 4: model evaluation (testing)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we're going to test the model by using it to predict the class labels of the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Model accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How well did our model perform? The simplest statistic we can compute is the \"classification accuracy\": in the simplest case, this is the fraction of files in our test set that were classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5894929749541845\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5894929749541845\n"
     ]
    }
   ],
   "source": [
    "# The accuracy is also very easy to compute \"manually\"\n",
    "my_accuracy = np.sum(predictions == np.asarray(test_labels)) / float(len(test_labels))\n",
    "print(my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Confusion matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the accuracy is only the tip of the iceberg! The model also made some mistakes - what got confused with what? To answer this we can plot the **confusion matrix**: the rows of the matrix represent the true label, and the columns represent the label predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[539 335]\n",
      " [312 451]]\n"
     ]
    }
   ],
   "source": [
    "# lets compute the show the confusion matrix:\n",
    "cm = sklearn.metrics.confusion_matrix(test_labels, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first row represents the classical class, the second row the rock class. The first column shows what the model predicted as classifical, and the second column what the model predicted as rock. This means the numbers along the diagonal of the matrix represent correct predictions. Here's what the matrix tells us:\n",
    " - top right: The model classified 9 classical files as classical (correct)\n",
    " - top left: The model classified 1 classical files as rock (mistakes)\n",
    " - bottom left: The model classified 0 rock files as classical (mistakes)\n",
    " - bottm right: The model classified 10 rock files as tock (correct)\n",
    " \n",
    "We can also visualize the confusion matrix in a slightly more visually informative way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAScAAAEKCAYAAABQaJOpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec320f83c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cm, interpolation='nearest', cmap='gray')\n",
    "for i, line in enumerate(cm):\n",
    "    for j, l in enumerate(line):\n",
    "        ax.text(j, i, l, size=20, color='green')\n",
    "ax.set_xticks(range(len(cm)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks(range(len(cm)))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since in this example we have 2 classes (labels), the confusion matrix is 2x2. Furthermore, the confusions are fairly obvious, classical can only be confused with rock, and rock can only be confused with classical. However, when working on a multiclass problem (i.e. when there are more than 2 classes), the confusion matrix can be much more informative, as it tells us how much each class is confused with every other class. This can help us identify particularly problematic classes that are confused often, and help us figure out how to improve the model: are the classes well represented by the training data or do we need more data? Are the features that we're using sufficient for distinguishing between these classes, or do we need more/different features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Qualitative analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By now we know how accurate our model is (on the test data), and how many files of each class get confused with another class. But we still don't know WHY the model made the specific mistakes that it did. Is it a limittation of the model? Is it because of the features we use? Perhaps it's the data itself? Answering this is a hard but necessary question. Sometimes we can try to figure this out quantitatively, but often we'll have to take a more qualitative approach.\n",
    "\n",
    "Let's start by listing the missclassified files and what they were classified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "index = 0\n",
    "for filename, prediction, label in zip(test_files, predictions, test_labels):\n",
    "    if prediction != label:\n",
    "        print \"{:d} {:s} is {:s} but was classified as {:s}\".format(\n",
    "            index, os.path.basename(filename), labels[label], labels[prediction])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since our data is audio, we can listen to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "audio_err, _ = librosa.load(test_files[1], sr=samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "Audio(audio_err, rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With any luck, through listening to some of the errors we can try to identify why they were missclassified. In this example the dataset is so small that the mistakes are likely due to the limited amount of data available to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for a specific music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: SweetLights_YouLetMeDown_MIX.wav\n",
      "mfcc matrix shape: (13, 67526)\n",
      "number of chunks 785\n",
      "file label size: (785,)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "tf = test_files[0]\n",
    "\n",
    "test_feat =[]\n",
    "test_labels = []\n",
    "print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "# Load audio\n",
    "audio, sr = librosa.load(tf, sr=samplerate, mono=True)\n",
    "\n",
    "# Extract mfcc coefficients (remember we will discard the first one)\n",
    "# To see all the relevant kwarg arugments consult the documentation for\n",
    "# librosa.feature.mfcc, librosa.feature.melspectrogram and librosa.filters.mel\n",
    "mfcc = librosa.feature.mfcc(audio, sr=sr, n_fft=window_size, hop_length=hop_size,\n",
    "                            fmax=samplerate/2, n_mels=n_bands, n_mfcc=(n_mfcc + 1))\n",
    "          \n",
    "# Discard the first coefficient\n",
    "mfcc = mfcc[1:,:]\n",
    "print(\"mfcc matrix shape: {}\".format(mfcc.shape))\n",
    "    \n",
    "    \n",
    "# Read labels for each frame\n",
    "f0line = pd.read_csv(tf[:-7]+\"MELODY1.csv\",index_col=None, header=None)\n",
    "f0line = pd.DataFrame.as_matrix(f0line)[:,1]\n",
    "    \n",
    "#print (mfcc.shape)\n",
    "print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "feature_vector = []\n",
    "tf_label = []\n",
    "    \n",
    "for chunk in range(int(mfcc.shape[1]/half_sec)):\n",
    "    start = chunk*half_sec\n",
    "    mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "    mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "    \n",
    "    # We could do the same for the delta features like this:\n",
    "    # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "    # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "    # Concatenate means and std. dev's into a single feature vector\n",
    "    feature_vector.append(np.concatenate((mfcc_means, mfcc_stddevs), axis=0))\n",
    "    #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "    # Adjust labels to our classes\n",
    "    if len([x for x in f0line[start:start+feature_length] if x > 0]) > half_sec:\n",
    "        tf_label.append('present')\n",
    "    else:\n",
    "        tf_label.append('abscent')\n",
    "    \n",
    "#Get labels index\n",
    "tf_label_ind = np.array([labels.index(lbl) for lbl in tf_label])\n",
    "print(\"file label size: {}\".format(tf_label_ind.shape))\n",
    "    \n",
    "    \n",
    "# Store the feature vector and corresponding label in integer format\n",
    "for idx in range(len(feature_vector)-1):\n",
    "    test_feat.append(feature_vector[idx])\n",
    "    test_labels.append(tf_label_ind[idx])\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_scaled = scaler.transform(test_feat)\n",
    "output = clf.predict(feat_scaled)\n",
    "labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.5531400966183575 False Alarme 0.32432432432432434\n"
     ]
    }
   ],
   "source": [
    "R, FA = mir_eval.melody.voicing_measures(np.array(labels), np.array(output)) \n",
    "print (\"Recall\", R, \"False Alarme\", FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70626"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(np.array(test_labels))\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reference and estimated voicing arrays should be the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-63e55558ed29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmir_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoicing_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"False Alarme\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/compmus/.local/lib/python2.7/site-packages/mir_eval/melody.pyc\u001b[0m in \u001b[0;36mvoicing_measures\u001b[0;34m(ref_voicing, est_voicing)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0mvalidate_voicing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_voicing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_voicing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0mref_voicing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_voicing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0mest_voicing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mest_voicing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/compmus/.local/lib/python2.7/site-packages/mir_eval/melody.pyc\u001b[0m in \u001b[0;36mvalidate_voicing\u001b[0;34m(ref_voicing, est_voicing)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Make sure they're the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mref_voicing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mest_voicing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         raise ValueError('Reference and estimated voicing arrays should '\n\u001b[0m\u001b[1;32m     83\u001b[0m                          'be the same length.')\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvoicing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mref_voicing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_voicing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Reference and estimated voicing arrays should be the same length."
     ]
    }
   ],
   "source": [
    "R, FA = mir_eval.melody.voicing_measures(np.array(test_labels), predictions) \n",
    "print (\"Recall\", R, \"False Alarme\", FA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
