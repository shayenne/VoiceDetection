{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>A Voice Activity Detector based on MFCC features</h1>\n",
    "\n",
    "This notebook shows how to build a Voice Activity classifier using MFCC features. \n",
    "\n",
    "**Why MFCC?**\n",
    "\n",
    "MFCC are a good starting point when building an audio classifier designed to distinguish between audio recordings based on their **timbre**, since they provide a compact representation of the spectral envelope. Examples of audio classses that are well characterized by their timbre include musical instruments (e.g. trumpet vs. piano) or urban environmental sounds (e.g. siren vs. car honk vs. and jackhammer).\n",
    "\n",
    "However, it is important to bear in mind that not all audio-related classification tasks are timbre-related. For example, you might be interested in identifying (classifiying) the chords played in a music recording. What makes one chord different from another is **not its timbre**: the same chord can be played by different instruments with different timbres and it will still be the same chord. What makes chords different (among other factors) are the notes (pitches) they contain. Thus, a more suitable class of features for chord classification would be [chroma features](https://en.wikipedia.org/wiki/Chroma_feature).\n",
    "\n",
    "Since in this example we want to distinguish between human voice presence and abscence on a song (genres), which (usually) have different timbre, MFCC are probably a good choice for the task.\n",
    "\n",
    "**IMPORTANT**: This example makes one assumption:\n",
    "\n",
    "1. All the audio files have been preprocessed to ensure they are in the same format: single channel (mono) with a sample rate of 44100 Hz and a bit depth of 16 in wav format. If you haven't preprocessed your audio, consult the **```convert_audio_format```** notebook in this repository. Working with audio files in different formats can lead to unexpected (=undesired!) results.\n",
    "\n",
    "\n",
    "Dependencies:\n",
    "* numpy: http://www.numpy.org/\n",
    "* scikit-learn: http://scikit-learn.org/\n",
    "* librosa: http://librosa.github.io/librosa/\n",
    "* matplotlib: http://matplotlib.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the modules we're going to need\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import pandas as pd# Added\n",
    "from IPython.display import Audio\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 0: Compiling your dataset</h2>\n",
    "\n",
    "For creating this example I've used a collection of songs with human singers. All the files have been preprocessed using the ```convert_audio_format``` and ```rename_files_predix``` notebooks, and then divided into a train and test folder. There are 13 files in the train folder and 4 files in the test folder.\n",
    "\n",
    "Since we will use different files in class (your music!), the results displayed in this notebook will be different when we run it in class. {?}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 1: Data cleaning</h2>\n",
    "\n",
    "As noted above, this notebook assumes data cleaning has already been performed: all files are in the same format (single channel [mono], 16-bit depth, 44100 sample rate, wav), and the class label of each file is indicated as a prefix followed by an underscore in the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We've previously preprocessed our data and coverted all files to a sample rate of 44100\n",
    "samplerate = 44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 2: Feature extraction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Specify where the audio files for training and testing reside\n",
    "train_folder = './exp1/train'\n",
    "test_folder = './exp1/test'\n",
    "\n",
    "# Get a list of all the training audio files (must be .WAV files)\n",
    "train_files = glob.glob(os.path.join(train_folder, '*.wav'))\n",
    "\n",
    "# Get a list of all the test audio files (must be .WAV files)\n",
    "test_files = glob.glob(os.path.join(test_folder, '*.wav'))\n",
    "\n",
    "# Specify the labels (classes) we're going to classify the data into\n",
    "label0 = 'abscent'\n",
    "label1 = 'present'\n",
    "labels = [label0, label1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to compute MFCC features using 40 mel bands, 40 DCT coefficients, keeping the bottom 13 MFCC coefficients after skipping the first coefficient (i.e. keep MFCC coefficieints 1-13, discard coefficients 0 and 14-39).\n",
    "\n",
    "Before training (and testing), we are going to standardize our features across each dimension (this is particularly important if you plan to use an SVM classifier for some kernel functions).\n",
    "\n",
    "**Tip 1**: we might be able to obtain better clasification by adding more features to our feature vector. For example, we could compute the delta (first derivative) of each MFCC coefficient time-series (```np.diff(mfcc)```) and compute summary statistics from this time series as well. We could even do the same for the delta-delta (second derivative). Another option would be to add more summary statistics such as the minimum, maximum, median, or higher-order moments such as skewness and kurtosis. Be careful though: if the dimensionality of your featue vector is too high you run the risk of overfitting your data, especially if your dataset is small!\n",
    "\n",
    "**Tip 2**: since the train/test sets used in this example are relatively small, and since we're only going to run through this example once, the features are computed on the fly in memory. If, however, you are working with a larger dataset (e.g. UrbanSound8K), feature extraction can take a considerable time. In this case, it's a good idea to store the features to disk, so that you can experiment with different classification models and classifier hyper-parameters without having to re-compute the features every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets start by defining the parameters of our MFCC features: \n",
    "* window (frame) size\n",
    "* hop size\n",
    "* number of mel bands (the number of DCT coefficients will be the same in this implementation) \n",
    "* number of MFCC coefficients to keep: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 2048\n",
    "hop_size = 441\n",
    "n_bands = 40\n",
    "n_mfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make 1 second summarization as features with half second of hop length\n",
    "# 172 frames == 1 second (using 44100 samples per second)\n",
    "feature_length = 96\n",
    "half_sec = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll extract features from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: StevenClark_Bounty_MIX.wav\n",
      "mfcc shape 28947\n",
      "number of chunks 603\n",
      "file label size: 603\n",
      " \n",
      "filename: HezekiahJones_BorrowedHeart_MIX.wav\n",
      "mfcc shape 24158\n",
      "number of chunks 503\n",
      "file label size: 503\n",
      " \n",
      "filename: TheDistricts_Vermont_MIX.wav\n",
      "mfcc shape 22816\n",
      "number of chunks 475\n",
      "file label size: 475\n",
      " \n",
      "filename: ClaraBerryAndWooldog_WaltzForMyVictims_MIX.wav\n",
      "mfcc shape 17544\n",
      "number of chunks 365\n",
      "file label size: 365\n",
      " \n",
      "filename: MusicDelta_80sRock_MIX.wav\n",
      "mfcc shape 3692\n",
      "number of chunks 76\n",
      "file label size: 76\n",
      " \n",
      "filename: Auctioneer_OurFutureFaces_MIX.wav\n",
      "mfcc shape 20789\n",
      "number of chunks 433\n",
      "file label size: 433\n",
      " \n",
      "filename: MusicDelta_Rockabilly_MIX.wav\n",
      "mfcc shape 2595\n",
      "number of chunks 54\n",
      "file label size: 54\n",
      " \n",
      "filename: ClaraBerryAndWooldog_Stella_MIX.wav\n",
      "mfcc shape 19575\n",
      "number of chunks 407\n",
      "file label size: 407\n",
      " \n",
      "filename: MatthewEntwistle_Lontano_MIX.wav\n",
      "mfcc shape 29801\n",
      "number of chunks 620\n",
      "file label size: 620\n",
      " \n",
      "filename: MusicDelta_Reggae_MIX.wav\n",
      "mfcc shape 1747\n",
      "number of chunks 36\n",
      "file label size: 36\n",
      " \n",
      "filename: MusicDelta_Grunge_MIX.wav\n",
      "mfcc shape 4186\n",
      "number of chunks 87\n",
      "file label size: 87\n",
      " \n",
      "filename: FacesOnFilm_WaitingForGa_MIX.wav\n",
      "mfcc shape 25764\n",
      "number of chunks 536\n",
      "file label size: 536\n",
      " \n",
      "filename: AlexanderRoss_VelvetCurtain_MIX.wav\n",
      "mfcc shape 51449\n",
      "number of chunks 1071\n",
      "file label size: 1071\n",
      " \n",
      "filename: Mozart_DiesBildnis_MIX.wav\n",
      "mfcc shape 22914\n",
      "number of chunks 477\n",
      "file label size: 477\n",
      " \n",
      "filename: MatthewEntwistle_DontYouEver_MIX.wav\n",
      "mfcc shape 11402\n",
      "number of chunks 237\n",
      "file label size: 237\n",
      " \n",
      "filename: DreamersOfTheGhetto_HeavyLove_MIX.wav\n",
      "mfcc shape 29499\n",
      "number of chunks 614\n",
      "file label size: 614\n",
      " \n",
      "filename: MusicDelta_Beatles_MIX.wav\n",
      "mfcc shape 3638\n",
      "number of chunks 75\n",
      "file label size: 75\n",
      " \n",
      "filename: HopAlong_SisterCities_MIX.wav\n",
      "mfcc shape 28343\n",
      "number of chunks 590\n",
      "file label size: 590\n",
      " \n",
      "filename: MusicDelta_Hendrix_MIX.wav\n",
      "mfcc shape 1985\n",
      "number of chunks 41\n",
      "file label size: 41\n",
      " \n",
      "filename: BigTroubles_Phantom_MIX.wav\n",
      "mfcc shape 14694\n",
      "number of chunks 306\n",
      "file label size: 306\n",
      " \n",
      "filename: TheScarletBrand_LesFleursDuMal_MIX.wav\n",
      "mfcc shape 30364\n",
      "number of chunks 632\n",
      "file label size: 632\n",
      " \n",
      "filename: PurlingHiss_Lolita_MIX.wav\n",
      "mfcc shape 25623\n",
      "number of chunks 533\n",
      "file label size: 533\n",
      " \n",
      "filename: BrandonWebster_DontHearAThing_MIX.wav\n",
      "mfcc shape 17166\n",
      "number of chunks 357\n",
      "file label size: 357\n",
      " \n",
      "filename: Debussy_LenfantProdigue_MIX.wav\n",
      "mfcc shape 22353\n",
      "number of chunks 465\n",
      "file label size: 465\n",
      " \n",
      "filename: AvaLuna_Waterduct_MIX.wav\n",
      "mfcc shape 25931\n",
      "number of chunks 540\n",
      "file label size: 540\n",
      " \n",
      "filename: ClaraBerryAndWooldog_TheBadGuys_MIX.wav\n",
      "mfcc shape 25469\n",
      "number of chunks 530\n",
      "file label size: 530\n",
      " \n",
      "filename: MusicDelta_Disco_MIX.wav\n",
      "mfcc shape 12478\n",
      "number of chunks 259\n",
      "file label size: 259\n",
      " \n",
      "filename: MusicDelta_Punk_MIX.wav\n",
      "mfcc shape 2877\n",
      "number of chunks 59\n",
      "file label size: 59\n",
      " \n",
      "filename: HeladoNegro_MitadDelMundo_MIX.wav\n",
      "mfcc shape 18186\n",
      "number of chunks 378\n",
      "file label size: 378\n",
      " \n",
      "filename: BrandonWebster_YesSirICanFly_MIX.wav\n",
      "mfcc shape 9965\n",
      "number of chunks 207\n",
      "file label size: 207\n",
      " \n",
      "filename: ClaraBerryAndWooldog_AirTraffic_MIX.wav\n",
      "mfcc shape 17345\n",
      "number of chunks 361\n",
      "file label size: 361\n",
      " \n",
      "filename: Creepoid_OldTree_MIX.wav\n",
      "mfcc shape 30221\n",
      "number of chunks 629\n",
      "file label size: 629\n",
      " \n",
      "filename: ClaraBerryAndWooldog_Boys_MIX.wav\n",
      "mfcc shape 15348\n",
      "number of chunks 319\n",
      "file label size: 319\n",
      " \n",
      "filename: AClassicEducation_NightOwl_MIX.wav\n",
      "mfcc shape 17145\n",
      "number of chunks 357\n",
      "file label size: 357\n",
      " \n",
      "filename: Schubert_Erstarrung_MIX.wav\n",
      "mfcc shape 17390\n",
      "number of chunks 362\n",
      "file label size: 362\n",
      " \n",
      "filename: LizNelson_ImComingHome_MIX.wav\n",
      "mfcc shape 17970\n",
      "number of chunks 374\n",
      "file label size: 374\n",
      " \n",
      "filename: FamilyBand_Again_MIX.wav\n",
      "mfcc shape 20129\n",
      "number of chunks 419\n",
      "file label size: 419\n",
      " \n",
      "filename: AlexanderRoss_GoodbyeBolero_MIX.wav\n",
      "mfcc shape 41882\n",
      "number of chunks 872\n",
      "file label size: 872\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the training features and corresponding training labels\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in train_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(tf, sr=samplerate, mono=True)\n",
    "\n",
    "    # Extract mfcc coefficients (remember we will discard the first one)\n",
    "    # To see all the relevant kwarg arugments consult the documentation for\n",
    "    # librosa.feature.mfcc, librosa.feature.melspectrogram and librosa.filters.mel\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_fft=window_size, hop_length=hop_size,\n",
    "                                fmax=samplerate/2, n_mels=n_bands, n_mfcc=(n_mfcc + 1))\n",
    "          \n",
    "    # Discard the first coefficient\n",
    "    mfcc = mfcc[1:,:]\n",
    "    \n",
    "    print(\"mfcc shape\", int(mfcc.shape[1]))\n",
    "    \n",
    "    # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"vocal.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)\n",
    "    f0line = f0line.T[0]\n",
    "    #timestamp = pd.DataFrame.as_matrix(f0line)[:,0]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "    # Delta features \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    # For half second\n",
    "    for chunk in range(int(mfcc.shape[1]/half_sec)):\n",
    "        start = chunk*half_sec\n",
    "        mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_max = np.max(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_median = np.median(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_d1_means = np.mean(mfcc_delta[:,start:start+feature_length], 1)\n",
    "        mfcc_d1_stddevs = np.std(mfcc_delta[:,start:start+feature_length], 1)\n",
    "        mfcc_d2_means = np.mean(mfcc_delta2[:,start:start+feature_length], 1)\n",
    "        mfcc_d2_stddevs = np.std(mfcc_delta2[:,start:start+feature_length], 1)\n",
    "        \n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(np.concatenate((mfcc_means, mfcc_stddevs, mfcc_max, mfcc_median,\\\n",
    "                                              mfcc_d1_means, mfcc_d1_stddevs, mfcc_d2_means, mfcc_d2_stddevs\\\n",
    "                                             ), axis=0))\n",
    "        #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) >= 2*half_sec: # Get 100% of frames\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "        \n",
    "    #tf_label = ['present' if x > 0.0 else 'abscent' for x in f0line]\n",
    "\n",
    "    # Get labels index\n",
    "    tf_label_ind = [labels.index(lbl) for lbl in tf_label]\n",
    "    print(\"file label size: {:d}\".format(len(tf_label_ind)))\n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)):\n",
    "        train_features.append(feature_vector[idx])\n",
    "        train_labels.append(tf_label_ind[idx]) \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, do do the same for the test data (of course we must extact exactly the same features for the training and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: MusicDelta_Country2_MIX.wav\n",
      "mfcc matrix shape: (13, 1746)\n",
      "number of chunks 36\n",
      "file label size: (36,)\n",
      " \n",
      "filename: MusicDelta_Country1_MIX.wav\n",
      "mfcc matrix shape: (13, 3475)\n",
      "number of chunks 72\n",
      "file label size: (72,)\n",
      " \n",
      "filename: SweetLights_YouLetMeDown_MIX.wav\n",
      "mfcc matrix shape: (13, 39199)\n",
      "number of chunks 816\n",
      "file label size: (816,)\n",
      " \n",
      "filename: MusicDelta_Gospel_MIX.wav\n",
      "mfcc matrix shape: (13, 7574)\n",
      "number of chunks 157\n",
      "file label size: (157,)\n",
      " \n",
      "filename: PortStWillow_StayEven_MIX.wav\n",
      "mfcc matrix shape: (13, 31702)\n",
      "number of chunks 660\n",
      "file label size: (660,)\n",
      " \n",
      "filename: StrandOfOaks_Spacestation_MIX.wav\n",
      "mfcc matrix shape: (13, 24387)\n",
      "number of chunks 508\n",
      "file label size: (508,)\n",
      " \n",
      "filename: MusicDelta_Rock_MIX.wav\n",
      "mfcc matrix shape: (13, 1310)\n",
      "number of chunks 27\n",
      "file label size: (27,)\n",
      " \n",
      "filename: InvisibleFamiliars_DisturbingWildlife_MIX.wav\n",
      "mfcc matrix shape: (13, 21870)\n",
      "number of chunks 455\n",
      "file label size: (455,)\n",
      " \n",
      "filename: Snowmine_Curfews_MIX.wav\n",
      "mfcc matrix shape: (13, 27520)\n",
      "number of chunks 573\n",
      "file label size: (573,)\n",
      " \n",
      "filename: CelestialShore_DieForUs_MIX.wav\n",
      "mfcc matrix shape: (13, 27868)\n",
      "number of chunks 580\n",
      "file label size: (580,)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Define lists to store the test features and corresponding test labels\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "# For every audio file in the training set, load the file, compute MFCCs, summarize them over time\n",
    "# using the mean and standard deviation (for each MFCC coefficient), and then save the features\n",
    "# and corresponding label in the designated lists\n",
    "for tf in test_files:\n",
    "    \n",
    "    print(\"filename: {:s}\".format(os.path.basename(tf)))\n",
    "    \n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(tf, sr=samplerate, mono=True)\n",
    "\n",
    "    # Extract mfcc coefficients (remember we will discard the first one)\n",
    "    # To see all the relevant kwarg arugments consult the documentation for\n",
    "    # librosa.feature.mfcc, librosa.feature.melspectrogram and librosa.filters.mel\n",
    "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_fft=window_size, hop_length=hop_size,\n",
    "                                fmax=samplerate/2, n_mels=n_bands, n_mfcc=(n_mfcc + 1))\n",
    "          \n",
    "    # Discard the first coefficient\n",
    "    mfcc = mfcc[1:,:]\n",
    "    print(\"mfcc matrix shape: {}\".format(mfcc.shape))\n",
    "    \n",
    "    # Read labels for each frame\n",
    "    f0line = pd.read_csv(tf[:-7]+\"vocal.csv\",index_col=None, header=None)\n",
    "    f0line = pd.DataFrame.as_matrix(f0line)\n",
    "    f0line = f0line.T[0]\n",
    "    \n",
    "    #print (mfcc.shape)\n",
    "    print(\"number of chunks\", int(mfcc.shape[1]/half_sec))\n",
    "    \n",
    "    feature_vector = []\n",
    "    tf_label = []\n",
    "    \n",
    "        # Delta features \n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    # For half second\n",
    "    for chunk in range(int(mfcc.shape[1]/half_sec)):\n",
    "        start = chunk*half_sec\n",
    "        mfcc_means = np.mean(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_stddevs = np.std(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_max = np.max(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_median = np.median(mfcc[:,start:start+feature_length], 1)\n",
    "        mfcc_d1_means = np.mean(mfcc_delta[:,start:start+feature_length], 1)\n",
    "        mfcc_d1_stddevs = np.std(mfcc_delta[:,start:start+feature_length], 1)\n",
    "        mfcc_d2_means = np.mean(mfcc_delta2[:,start:start+feature_length], 1)\n",
    "        mfcc_d2_stddevs = np.std(mfcc_delta2[:,start:start+feature_length], 1)\n",
    "    \n",
    "        # We could do the same for the delta features like this:\n",
    "        # mfcc_d1_means = np.mean(np.diff(mfcc), 1)\n",
    "        # mfcc_d1_stddevs = np.std(np.diff(mfcc), 1)\n",
    "    \n",
    "        # Concatenate means and std. dev's into a single feature vector\n",
    "        feature_vector.append(np.concatenate((mfcc_means, mfcc_stddevs, mfcc_max, mfcc_median,\\\n",
    "                                              mfcc_d1_means, mfcc_d1_stddevs, mfcc_d2_means, mfcc_d2_stddevs\\\n",
    "                                             ), axis=0))\n",
    "        #print(\"feature summary: {}\".format(len(feature_vector)))\n",
    "   \n",
    "        # Adjust labels to our classes\n",
    "        if len([x for x in f0line[start:start+feature_length] if x > 0]) >= 2*half_sec: # 100% of frames\n",
    "            tf_label.append('present')\n",
    "        else:\n",
    "            tf_label.append('abscent')\n",
    "    \n",
    "    #Get labels index\n",
    "    tf_label_ind = np.array([labels.index(lbl) for lbl in tf_label])\n",
    "    print(\"file label size: {}\".format(tf_label_ind.shape))\n",
    "    \n",
    "    \n",
    "    # Store the feature vector and corresponding label in integer format\n",
    "    for idx in range(len(feature_vector)):\n",
    "        test_features.append(feature_vector[idx])\n",
    "        test_labels.append(tf_label_ind[idx])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we'll appply some post-processing in the form of feature standardization. In our case we're going to standardize across dimensions: this means we compare the feature vector of all files **in the training set** and for each dimension we subtract the mean and divide by the standard deviation across all feature vectors.\n",
    "\n",
    "**Important**: it is incorrect to standardize the entire dataset (training and test) in one go, because that means we've \"looked\" at the test data. The correct way to perform standardization is to learn the parameters (mean/std. dev) from the training set only, and then apply exactly the same process to the test set (without examining the data). This can be accomplished easily using scikit-learn's StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scale object\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Learn the parameters from the training data only\n",
    "scaler.fit(train_features)\n",
    "\n",
    "# Apply the learned parameters to the training and test sets:\n",
    "train_features_scaled = scaler.transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "# Note, the first 2 operations (learning the standardization parameters from the training data \n",
    "# and applying them to the the training data) can be performed in one line using:\n",
    "# train_features_scaled = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.sav']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the scaler to disk\n",
    "filename = 'scaler.sav'\n",
    "joblib.dump(scaler, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> Step 3: model training</h2>\n",
    "\n",
    "Now that all of our features are computed, we can train a clasification model! In this example we're going to use the following model: the support vector machine classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_model_SVM_100.sav\n",
      "0.807174160039\n",
      "finalized_model_SVM_10.sav\n",
      "0.813017367311\n",
      "finalized_model_SVM_1.sav\n",
      "0.842558026294\n",
      "finalized_model_SVM_0.1.sav\n",
      "0.846128875183\n",
      "finalized_model_SVM_0.01.sav\n",
      "0.81090732024\n",
      "finalized_model_SVM_0.001.sav\n",
      "0.657847751988\n",
      "finalized_model_SVM_0.0001.sav\n",
      "0.657847751988\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to train a model with the training features we've extracted\n",
    "\n",
    "# Lets use a SVC with folowing C parameters: \n",
    "params = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "for c in params:\n",
    "    clf = sklearn.svm.SVC(C=c)\n",
    "\n",
    "    # Fit (=train) the model\n",
    "    clf.fit(train_features_scaled, train_labels)\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = 'finalized_model_SVM_'+str(c)+'.sav'\n",
    "    print (filename)\n",
    "    joblib.dump(clf, filename)\n",
    "    \n",
    "    # Now lets predict the labels of the test data!\n",
    "    predictions = clf.predict(test_features_scaled)\n",
    "    \n",
    "    # We can use sklearn to compute the accuracy score\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to train a model with the training features we've extracted\n",
    "\n",
    "# Lets use a SVC with default parameters: kernel RBF \n",
    "clf = sklearn.svm.SVC()\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf.fit(train_features_scaled, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 4: model evaluation (testing)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we're going to test the model by using it to predict the class labels of the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf.predict(test_features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Model accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How well did our model perform? The simplest statistic we can compute is the \"classification accuracy\": in the simplest case, this is the fraction of files in our test set that were classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The accuracy is also very easy to compute \"manually\"\n",
    "my_accuracy = np.sum(predictions == np.asarray(test_labels)) / float(len(test_labels))\n",
    "print(my_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Confusion matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the accuracy is only the tip of the iceberg! The model also made some mistakes - what got confused with what? To answer this we can plot the **confusion matrix**: the rows of the matrix represent the true label, and the columns represent the label predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# lets compute the show the confusion matrix:\n",
    "cm = sklearn.metrics.confusion_matrix(test_labels, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first row represents the classical class, the second row the rock class. The first column shows what the model predicted as classifical, and the second column what the model predicted as rock. This means the numbers along the diagonal of the matrix represent correct predictions. Here's what the matrix tells us:\n",
    " - top right: The model classified 9 classical files as classical (correct)\n",
    " - top left: The model classified 1 classical files as rock (mistakes)\n",
    " - bottom left: The model classified 0 rock files as classical (mistakes)\n",
    " - bottm right: The model classified 10 rock files as tock (correct)\n",
    " \n",
    "We can also visualize the confusion matrix in a slightly more visually informative way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cm, interpolation='nearest', cmap='gray')\n",
    "for i, line in enumerate(cm):\n",
    "    for j, l in enumerate(line):\n",
    "        ax.text(j, i, l, size=20, color='green')\n",
    "ax.set_xticks(range(len(cm)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks(range(len(cm)))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since in this example we have 2 classes (labels), the confusion matrix is 2x2. Furthermore, the confusions are fairly obvious, classical can only be confused with rock, and rock can only be confused with classical. However, when working on a multiclass problem (i.e. when there are more than 2 classes), the confusion matrix can be much more informative, as it tells us how much each class is confused with every other class. This can help us identify particularly problematic classes that are confused often, and help us figure out how to improve the model: are the classes well represented by the training data or do we need more data? Are the features that we're using sufficient for distinguishing between these classes, or do we need more/different features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier without scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalized_model_RF_100.sav\n",
      "0.831520857004\n",
      "finalized_model_RF_200.sav\n",
      "0.829897743873\n",
      "finalized_model_RF_500.sav\n",
      "0.832170102256\n",
      "finalized_model_RF_1000.sav\n",
      "0.830546989125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "params = [100, 200, 500, 1000]\n",
    "\n",
    "for estimators in params:\n",
    "    clf3 = RandomForestClassifier(n_estimators=estimators)\n",
    "\n",
    "    # Fit (=train) the model\n",
    "    clf3.fit(train_features, train_labels)\n",
    "    \n",
    "    # save the model to disk\n",
    "    filename = 'finalized_model_RF_'+str(estimators)+'.sav'\n",
    "    print (filename)\n",
    "    joblib.dump(clf3, filename)\n",
    "    \n",
    "    # Now lets predict the labels of the test data!\n",
    "    predictions = clf3.predict(test_features)\n",
    "    # We can use sklearn to compute the accuracy score\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf3 = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit (=train) the model\n",
    "clf3.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Step 4: model evaluation Random Forest Classifier (testing)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we're going to test the model by using it to predict the class labels of the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf3.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Model accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How well did our model perform? The simplest statistic we can compute is the \"classification accuracy\": in the simplest case, this is the fraction of files in our test set that were classified correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Confusion matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But the accuracy is only the tip of the iceberg! The model also made some mistakes - what got confused with what? To answer this we can plot the **confusion matrix**: the rows of the matrix represent the true label, and the columns represent the label predicted by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# lets compute the show the confusion matrix:\n",
    "cm = sklearn.metrics.confusion_matrix(test_labels, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cm, interpolation='nearest', cmap='gray')\n",
    "for i, line in enumerate(cm):\n",
    "    for j, l in enumerate(line):\n",
    "        ax.text(j, i, l, size=20, color='green')\n",
    "ax.set_xticks(range(len(cm)))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks(range(len(cm)))\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testing with test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.846128875183\n"
     ]
    }
   ],
   "source": [
    "# Load trained model (SVM)\n",
    "filename = 'finalized_model_SVM_0.1.sav' \n",
    "# load the model from disk\n",
    "clf = joblib.load(filename)\n",
    "\n",
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf.predict(test_features_scaled)\n",
    "\n",
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751287332647\n"
     ]
    }
   ],
   "source": [
    "# Load trained model (RF)\n",
    "filename = 'finalized_model_RF_500.sav' \n",
    "# load the model from disk\n",
    "clf3 = joblib.load(filename)\n",
    "\n",
    "# Now lets predict the labels of the test data!\n",
    "predictions = clf3.predict(test_features)\n",
    "# We can use sklearn to compute the accuracy score\n",
    "accuracy = sklearn.metrics.accuracy_score(test_labels, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
